---
title: "Monitoring"
description: "Centralized logging with Graylog, uptime monitoring, alerting, and observability"
tags: ["wiki"]
topics: ["monitoring", "graylog", "logging", "alerting", "observability"]
---

Visibility into 50+ services requires centralized logging, proactive alerting, and dashboards. This wiki covers my monitoring stack and the patterns that make it work.

## Monitoring Stack

{{< mermaid >}}
graph TB
    subgraph "Log Sources"
        FW[Firewall Logs<br/>Syslog TCP]
        DNS[Pi-hole DNS<br/>Syslog UDP]
        DOCKER[Docker Containers<br/>GELF UDP]
        NAS[NAS Devices<br/>Syslog UDP]
    end

    subgraph "Log Pipeline"
        GRAYLOG[Graylog<br/>Ingest + Process]
        OPENSEARCH[OpenSearch<br/>Storage + Search]
        MONGODB[MongoDB<br/>Configuration]
    end

    subgraph "Visualization"
        DASH[Graylog Dashboards]
        PULSE[Pulse<br/>Proxmox Metrics]
        UTK[Uptime Kuma<br/>Availability]
    end

    subgraph "Alerting"
        DISCORD[Discord Webhooks]
    end

    FW & DNS & DOCKER & NAS --> GRAYLOG
    GRAYLOG --> OPENSEARCH
    GRAYLOG --> MONGODB
    GRAYLOG --> DASH
    DASH & PULSE & UTK --> DISCORD
{{< /mermaid >}}

## Graylog Centralized Logging

[Graylog](https://graylog.org) is my log aggregation platformâ€”collecting, processing, and visualizing logs from across the homelab.

### Architecture

| Component | Purpose | Resources |
|-----------|---------|-----------|
| **Graylog** | Web UI + log ingestion | 1 GB JVM heap |
| **OpenSearch** | Log storage + full-text search | 1 GB JVM heap |
| **MongoDB** | Configuration metadata | ~200 MB |

**Storage**: Dedicated 240 GB disk for OpenSearch indices.

**Key insight**: JVM heap tuning matters. Leaving ~2 GB free for OS filesystem cache dramatically improves OpenSearch query performance.

### Log Shipping Methods

| Transport | Port | Use Case | Example |
|-----------|------|----------|---------|
| **Syslog TCP** | 1514 | Appliance native syslog | Firewall traffic logs |
| **Syslog UDP** | 1514 | rsyslog forwarding | Pi-hole DNS, NAS |
| **GELF UDP** | 12201 | Docker container logs | Caddy, other stacks |

### GELF Docker Pattern

Docker services ship logs directly to Graylog:

```yaml
services:
  myservice:
    logging:
      driver: gelf
      options:
        gelf-address: "udp://<GRAYLOG_IP>:12201"
        tag: "service-name"
```

**Benefits:**
- Structured log fields (container name, image, etc.)
- No log rotation management
- `docker logs` still works (Docker dual logging)

**Gotcha**: Changing logging driver requires `docker compose down && up -d`, not just restart.

### rsyslog Forwarding Pattern

For services without native Graylog support, rsyslog forwards logs:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Application Log â”‚
â”‚ /var/log/app.logâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rsyslog imfile  â”‚ â† Tail log file
â”‚ (file input)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ rsyslog omfwd   â”‚ â†’ Forward to Graylog
â”‚ (UDP 1514)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example**: Pi-hole DNS logs â†’ rsyslog â†’ Graylog â†’ Dashboard.

### Active Pipelines

Each log source has a processing pipeline that extracts structured fields:

| Stream | Source | Field Prefix | Dashboards |
|--------|--------|--------------|------------|
| Pi-hole DNS | DNS servers | `dns_` | Query analysis, blocked domains |
| PAN-OS Firewall | Main firewall | `fw_` | Traffic, threats, blocked activity |
| Synology NAS | NAS devices | `syn_` | Storage, access logs |
| Caddy Proxy | Reverse proxy | `caddy_` | Request analysis |
| Internet Modem | Upstream modem | `modem_` | Connection logs |

### Pipeline Design Pattern

All pipelines use a **single-stage pattern**:

```
Stage 0 (match either)
â”œâ”€â”€ Rule 1: Parse field A
â”œâ”€â”€ Rule 2: Parse field B
â”œâ”€â”€ Rule 3: Parse field C
â””â”€â”€ Catch-all: Tag unmatched
```

**Why single stage?** Graylog has a bug where `match either` in stage 0 prevents stage 1 from executing when no rules match. Single-stage with content-based exclusions avoids this.

### Index Retention

All index sets use standardized retention:

| Setting | Value |
|---------|-------|
| Min Lifetime | 30 days |
| Max Lifetime | 90 days |
| Strategy | Time-based with size optimization |
| Deletion | Automatic after max lifetime |

**Rationale**: 30-90 days covers most troubleshooting needs. Older logs rarely needed; if they are, PBS backups have the original sources.

## Dashboards

### Graylog Dashboards

13 dashboards organized by function:

| Category | Dashboards | Purpose |
|----------|------------|---------|
| **DNS** | Pi-hole Overview, DNS Security, DNS Operations | Query analysis, blocked domains |
| **Firewall** | Traffic, Threats, Blocked, URLs, Network Activity | Security visibility |
| **Infrastructure** | Homelab Security Overview | Cross-service summary |
| **Proxy** | Caddy Overview | Request analysis |

**Dashboard creation**: Python scripts using the Graylog REST API create dashboards programmatically. This enables version control and reproducible deployments.

### Pulse (Proxmox Monitoring)

[Pulse](https://github.com/relliky/proxmox-pulse) provides Proxmox-specific metrics:

| Metric | Visualization |
|--------|---------------|
| CPU usage (per node) | Time series graph |
| RAM usage (per node) | Time series graph |
| Disk usage | Bar charts |
| Network I/O | Time series graph |
| VM/LXC status | Status cards |

**Integration**: Read-only Proxmox API user (`pulse@pve` with PVEAuditor role).

## Uptime Monitoring

### Uptime Kuma

Two Uptime Kuma instances for redundant availability monitoring:

| Instance | Purpose |
|----------|---------|
| UTK-A | Primary monitoring |
| UTK-B | Secondary (monitors UTK-A too) |

**Why two?** If UTK-A goes down, UTK-B notices and alerts. Single instance = blind spot.

### Monitor Types

| Type | Use Case | Example |
|------|----------|---------|
| **HTTP(S)** | Web services | Graylog UI, Proxmox API |
| **TCP Port** | Raw connectivity | SSH, database ports |
| **Ping** | Basic availability | Network devices |
| **DNS** | Resolution check | Pi-hole health |

### Monitor Strategy

```
Critical Services (1 min interval)
â”œâ”€â”€ DNS VIP
â”œâ”€â”€ Reverse Proxy VIP
â”œâ”€â”€ Main Firewall
â””â”€â”€ NAS devices

Standard Services (5 min interval)
â”œâ”€â”€ All web UIs
â”œâ”€â”€ API endpoints
â””â”€â”€ Docker hosts

Low Priority (15 min interval)
â””â”€â”€ Development containers
```

## Alerting

### Discord Integration

All monitoring sends alerts to Discord:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ðŸ”´ Service Down                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Service: DNS Primary                 â”‚
â”‚ Status: DOWN                         â”‚
â”‚ Duration: 2m 15s                     â”‚
â”‚ Last Check: 10:42:15 AM              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [View Dashboard]                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Alert Sources

| Source | Alert Types |
|--------|-------------|
| **Uptime Kuma** | Service down/up, certificate expiry |
| **Graylog** | Log-based alerts (high error rate, security events) |
| **Keepalived** | HA failover notifications |
| **Watchtower** | Container updates |
| **Backup scripts** | Backup success/failure |

### Alert Fatigue Prevention

**Strategies to avoid noise:**

1. **Severity tiers**: Only critical alerts wake me up
2. **Cooldown periods**: 30-minute minimum between repeat alerts
3. **Flap detection**: Ignore rapid up/down cycles (usually transient)
4. **Maintenance windows**: Suppress alerts during planned work

## Network Discovery

### Pi.Alert

Scans subnets every 5 minutes for device discovery:

| Feature | Purpose |
|---------|---------|
| New device detection | Alert on unknown devices |
| MAC tracking | Identify device moves |
| Port scanning | Discover services |
| Vendor lookup | Identify device types |

**Monitored subnets**: Management VLAN, Server VLAN.

## Observability Patterns

### Log-Based Monitoring

```
Application â†’ Graylog â†’ Stream â†’ Alert Condition â†’ Discord
```

**Example**: "More than 10 DNS query failures in 5 minutes" â†’ Alert.

### Metric-Based Monitoring

```
Service â†’ Prometheus/API â†’ Pulse/Dashboard â†’ Threshold â†’ Alert
```

**Example**: "Proxmox node CPU > 90% for 5 minutes" â†’ Alert.

### Synthetic Monitoring

```
Uptime Kuma â†’ Scheduled Request â†’ Service â†’ Response Check â†’ Alert
```

**Example**: "Graylog /api/system/lbstatus doesn't return ALIVE" â†’ Alert.

## Lessons Learned

### 1. JVM Heap Pinning

Graylog and OpenSearch both run on JVMs. Without explicit heap limits, they compete for RAM and starve the OS filesystem cache. **Always pin JVM heap:**

```yaml
GRAYLOG_SERVER_JAVA_OPTS: "-Xms1g -Xmx1g"
OPENSEARCH_JAVA_OPTS: "-Xms1g -Xmx1g"
```

### 2. Graylog 7 API Gotchas

Graylog 7 changed its REST API significantly:

| Issue | Solution |
|-------|----------|
| `entity cannot be null` | Wrap body in `{"entity": <payload>}` |
| Regex capture groups | 0-indexed: `m["0"]` = first group |
| Stream creation | Requires explicit `index_set_id` |
| Dashboard series format | Use search_type format, not widget format |

### 3. Log Retention Balance

Too short = missing data when you need it.
Too long = disk full, slow queries.

30-90 days is the sweet spot for homelab scale.

### 4. Redundant Monitoring

The monitoring system itself needs monitoring. UTK-B watching UTK-A ensures no blind spots.

### 5. Pipeline Rules Only Apply at Ingestion

Graylog pipeline rules process messages **at ingestion time only**. Changing a rule doesn't reprocess existing logs. For historical data, you must re-send the logs or query raw fields.

## Quick Reference

### Graylog API

```bash
# Health check
curl http://<GRAYLOG_IP>:9000/api/system/lbstatus

# List streams
curl -u admin:<PASSWORD> http://<GRAYLOG_IP>:9000/api/streams

# Search logs
curl -u admin:<PASSWORD> "http://<GRAYLOG_IP>:9000/api/views/search/messages?query=source:firewall"
```

### Log Shipping Test

```bash
# Test syslog UDP
echo "<14>Test message from $(hostname)" | nc -u <GRAYLOG_IP> 1514

# Test GELF
echo '{"short_message":"Test","host":"test"}' | nc -u <GRAYLOG_IP> 12201
```

## Related Pages

- [Networking]({{< relref "/wiki/networking" >}}) - Log sources (firewall, DNS)
- [Virtualization]({{< relref "/wiki/virtualization" >}}) - Where monitoring runs
- [Automation]({{< relref "/wiki/automation" >}}) - Alert automation
